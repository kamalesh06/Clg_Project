{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcxx9akSyuEe0yPgcMu444",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalesh06/Clg_Project/blob/main/untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCpQWmhu4XNM"
      },
      "outputs": [],
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "import whisper\n",
        "from TTS.api import TTS\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import sqlite3\n",
        "import os\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import torch\n",
        "import tempfile\n",
        "import soundfile as sf  # Fix for saving audio files\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "# Load Whisper model for Speech-to-Text\n",
        "@st.cache_resource\n",
        "def load_whisper_model():\n",
        "    return whisper.load_model(\"small\")\n",
        "\n",
        "# Load Coqui TTS for Text-to-Speech\n",
        "@st.cache_resource\n",
        "def load_coqui_tts():\n",
        "    return TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False)\n",
        "\n",
        "# Load Mistral-7B Model (Optimized for M1 Mac)\n",
        "@st.cache_resource\n",
        "def load_llm():\n",
        "    model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32 if device == \"mps\" else torch.float16,  # Ensure MPS compatibility\n",
        "        device_map=\"cpu\" if device == \"mps\" else \"auto\",  # Force CPU for MPS\n",
        "    )\n",
        "\n",
        "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device != \"mps\" else -1)\n",
        "\n",
        "# Initialize SQLite database for storing interactions\n",
        "def init_db():\n",
        "    conn = sqlite3.connect('mahabalipuram.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS interactions\n",
        "                 (id INTEGER PRIMARY KEY, user_input TEXT, bot_response TEXT, language TEXT)''')\n",
        "    conn.commit()\n",
        "    return conn, c\n",
        "\n",
        "# Generate bot response\n",
        "def generate_response(user_input):\n",
        "    generator = load_llm()\n",
        "    response = generator(user_input, max_new_tokens=100)  # Prevent cutting off mid-sentence\n",
        "    return response[0]['generated_text'].strip()\n",
        "\n",
        "# Convert audio frame to text using Whisper\n",
        "def process_audio(audio_file, whisper_model):\n",
        "    # Load audio file\n",
        "    audio, sr = sf.read(audio_file)\n",
        "\n",
        "    # Ensure correct sample rate using torchaudio\n",
        "    target_sr = 16000\n",
        "    waveform = torch.tensor(audio).float()\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
        "    resampled_audio = resampler(waveform).numpy()\n",
        "\n",
        "    # Save resampled audio to temporary file using `soundfile`\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp_audio:\n",
        "        sf.write(tmp_audio.name, resampled_audio, target_sr)\n",
        "        audio_file = tmp_audio.name\n",
        "\n",
        "    # Transcribe with Whisper\n",
        "    result = whisper_model.transcribe(audio_file)\n",
        "    os.remove(audio_file)  # Cleanup\n",
        "    return result[\"text\"]\n",
        "\n",
        "def main():\n",
        "    st.title(\"Mahabalipuram AI Voice Bot\")\n",
        "\n",
        "    st.write(\"Welcome to the Mahabalipuram Heritage Assistant! Speak into your microphone or upload an audio file.\")\n",
        "\n",
        "    # Language selection\n",
        "    language = st.selectbox(\"Select Language\", [\"Tamil\", \"English\", \"Hindi\"])\n",
        "\n",
        "    # Audio input\n",
        "    st.write(\"### Record a voice message\")\n",
        "    audio_file = st.audio_input(\"Record a voice message\")\n",
        "\n",
        "    # Load models and database\n",
        "    whisper_model = load_whisper_model()\n",
        "    tts = load_coqui_tts()\n",
        "    conn, cursor = init_db()\n",
        "\n",
        "    if audio_file:\n",
        "        st.write(\"Processing audio...\")\n",
        "        try:\n",
        "            user_input = process_audio(audio_file, whisper_model)\n",
        "            st.write(f\"**You said:** {user_input}\")\n",
        "\n",
        "            bot_response = generate_response(user_input)\n",
        "            st.write(f\"**Bot Response (English):** {bot_response}\")\n",
        "\n",
        "            # Store interaction in the database\n",
        "            cursor.execute(\"INSERT INTO interactions (user_input, bot_response, language) VALUES (?, ?, ?)\",\n",
        "                           (user_input, bot_response, language))\n",
        "            conn.commit()\n",
        "\n",
        "            # Convert response to speech\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp_audio:\n",
        "                tts.tts_to_file(text=bot_response, file_path=tmp_audio.name)\n",
        "                audio_file = tmp_audio.name\n",
        "\n",
        "            st.audio(audio_file)\n",
        "\n",
        "            # Defer deletion of audio file until next session state reset\n",
        "            st.session_state[\"last_audio_file\"] = audio_file\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run main.py"
      ],
      "metadata": {
        "id": "NDkqtpDv7G3p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}